{"cells":[{"cell_type":"markdown","metadata":{"id":"TkY-AsX9fdjx"},"source":["Build out triplets for the SBert Finetuning for minimizing triplet loss\n","Create Separate TF/IDF models for Entities Lemmas and Body Tokens for use on corresponding vectorization\n","\n","Train the model with the instances to get the learned weights to use for clustering\n","\n","<b> This notebook relies on the build_repclusters notebook to create the necessary representations from the test set</b>"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"QzEhPNmgfdj2","executionInfo":{"status":"ok","timestamp":1659736604754,"user_tz":240,"elapsed":115,"user":{"displayName":"Nadia Conroy","userId":"09687839967270010584"}}},"outputs":[],"source":["import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xyH_gt3Ih1n9","executionInfo":{"status":"ok","timestamp":1659736606622,"user_tz":240,"elapsed":753,"user":{"displayName":"Nadia Conroy","userId":"09687839967270010584"}},"outputId":"82ab4329-545d-488e-eef6-4405b073f7d7"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","execution_count":7,"metadata":{"id":"aTKwKtywfdj4","executionInfo":{"status":"ok","timestamp":1659736607924,"user_tz":240,"elapsed":100,"user":{"displayName":"Nadia Conroy","userId":"09687839967270010584"}}},"outputs":[],"source":["import sys\n","sys.path.insert(1, './drive/MyDrive/EventClustering/src')\n"]},{"cell_type":"code","source":["#!pip install transformers\n","#!pip install bert-tensorflow\n","!pip install tensorflow"],"metadata":{"id":"7DZKyj5Uirjx"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"8Ye3MhuIfdj5","executionInfo":{"status":"error","timestamp":1659736610224,"user_tz":240,"elapsed":353,"user":{"displayName":"Nadia Conroy","userId":"09687839967270010584"}},"outputId":"913b6d2c-91ee-4181-861b-fd3ff23bdc1b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Pytorch Version: 1.12.0+cu113\n"]},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-969f61393f74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcluster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/EventClustering/src/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0mpretrained_bertpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./models/BertFinetune/clustFT-bert-base-uncased\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_bertpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_LEN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0mbert_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertForSequenceClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_bertpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0;31m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 534\u001b[0;31m         \u001b[0mtokenizer_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_tokenizer_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    535\u001b[0m         \u001b[0mconfig_tokenizer_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tokenizer_class\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m         \u001b[0mtokenizer_auto_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mget_tokenizer_config\u001b[0;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, **kwargs)\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m         \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m         \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m     )\n\u001b[1;32m    403\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresolved_config_file\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mget_file_from_repo\u001b[0;34m(path_or_repo, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only)\u001b[0m\n\u001b[1;32m    709\u001b[0m             \u001b[0mresume_download\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_download\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 711\u001b[0;31m             \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    712\u001b[0m         )\n\u001b[1;32m    713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m    290\u001b[0m             \u001b[0muser_agent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muser_agent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m             \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m         )\n\u001b[1;32m    294\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_or_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m    561\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m                     raise ValueError(\n\u001b[0;32m--> 563\u001b[0;31m                         \u001b[0;34m\"Connection error, and we cannot find the requested files in the cached path.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m                         \u001b[0;34m\" Please try again or make sure your Internet connection is on.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m                     )\n","\u001b[0;31mValueError\u001b[0m: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on."]}],"source":["import pandas as pd\n","import utils\n","import corpus\n","import cluster\n","import model\n","\n","pd.set_option('display.max_rows', None)\n","pd.set_option('display.max_columns', None)\n","pd.set_option('display.width', None)\n","pd.set_option('display.max_colwidth', -1)\n","\n","lang='eng'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nFqxVcbvfdj6"},"outputs":[],"source":["lang = 'eng'\n","test_corpus = corpus.Corpus()\n","test_corpus.load_corpora(r\"dataset/dataset.dev.json\",\n","                           r\"dataset/clustering.dev.json\", set([lang]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v7Jn3UEZfdj6"},"outputs":[],"source":["print(lang,\"#docs\",len(test_corpus.documents))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FCi2RLHbfdj7"},"outputs":[],"source":["df_testcorpus = pd.DataFrame(test_corpus.documents)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZHBV_j-Qfdj8"},"outputs":[],"source":["input_data = test_corpus.documents \n","utils.random.shuffle(input_data)\n","#input_data = input_data[:2500]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0BLk9VXzfdj8"},"outputs":[],"source":["df_clusters=df_testcorpus.sort_values(['id','text'], ascending =[1,0])\n","df_clusters = df_clusters.groupby('cluster').filter(lambda x : len(x)>2)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p9V8YARofdj9"},"outputs":[],"source":["print(len(df_clusters))\n","print ((df_clusters.head()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-_l1IxWyfdj-"},"outputs":[],"source":["clusters = df_clusters.cluster.unique()\n","print (len(clusters))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ePC9xVjwfdj-"},"outputs":[],"source":["%%capture\n","from tqdm import tqdm_notebook as tqdm\n","tqdm().pandas()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fskvuaxCfdj-"},"outputs":[],"source":["instances = []\n","#For every cluster in the training set (using as subset for testing)\n","#Build the cluster representations of the averages of its documents\n","\n","with tqdm(total=len(clusters)) as pbar:\n","\n","  for c in (clusters):\n","    \n","    #Generate the list of documents for that cluster and create the full cluster representation\n","    doclist = df_clusters.loc[df_clusters['cluster'] == c]\n","    assert (len(doclist) > 2 )\n","    print ('Assembling list of ', len(doclist), ' documents in the cluster #', c)\n"," \n","    first_doc = corpus.Document(doclist.iloc[0])\n","    doclist = doclist.iloc[1:]\n","    current_cluster=cluster.Cluster(first_doc, c)\n","    \n","    doc_reps = []\n","    for index, row in doclist.iterrows():\n","        current_doc = corpus.Document(row)\n","        doc_reps.append(current_doc)\n","        current_cluster.add_document(current_doc)\n","        \n","    \n","    #Select a random negative cluster number not the current one\n","    neg_clusternum = utils.random.choice(clusters)  \n","    while (neg_clusternum == c):\n","        neg_clusternum = utils.random.choice(clusters)\n","      \n","    #Generate the list of documents for that negative cluster and generate the cluster representation\n","    negdoclist = df_clusters.loc[df_clusters['cluster'] == neg_clusternum]\n","    assert (len(negdoclist) > 2 )\n","    print ('Assembling list of ', len(negdoclist), ' negative example documents in the cluster #', neg_clusternum)\n"," \n","    first_negdoc = corpus.Document(negdoclist.iloc[0])\n","    negdoclist = negdoclist.iloc[1:]\n","    current_negcluster=cluster.Cluster(first_negdoc, neg_clusternum)\n","    \n","    for index, row in negdoclist.iterrows():\n","        current_negdoc = corpus.Document(row)\n","        current_negcluster.add_document(current_doc)\n"," \n","      \n","    #For each document in the new cluster create a loss triplet using the current cluster and a negative example cluster \n","    for anchor in doc_reps:\n","        \n","        #Create the similarity vector with current cluster\n","        print ('\\nPostive sample with cluster ',current_cluster.reprs['cluster'],' and anchor doc cluster ', anchor.reprs['cluster'])\n","        sims = utils.sim_reps_dc(anchor, current_cluster) \n","        sim_vec = utils.create_vec(sims)\n","        \n","         \n","        #Create the similarity vector with negative cluster\n","        print ('Negative sample with cluster ',current_negcluster.reprs['cluster'],' and anchor doc cluster ', anchor.reprs['cluster'])\n","        difs = utils.sim_reps_dc(anchor, current_negcluster)\n","        dif_vec = utils.create_vec(difs)\n","\n","        loss = utils.np.subtract(sim_vec, dif_vec)\n","        print ('Their difference: ', loss)\n","        instances.append([loss, 1])\n","        \n","    #cluster_reps.append(current_cluster)\n","    pbar.update(1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pheKThCNfdj_"},"outputs":[],"source":["print (instances[:20])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_ZzhnBDkfdkA"},"outputs":[],"source":["#SAVE THE TEST CLUSTER REPRESENTATIONS WITH PICKLE\n","with open('POC/models/losstraining_allAug2.pkl','wb') as f:\n","    utils.pickle.dump(instances, f)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yfx1yNi3fdkA"},"outputs":[],"source":["print (training_reps[3].reprs['cluster'])\n","\n","print (cluster_reps[3].__dict__['oldest_timestamp'])\n","print (cluster_reps[3].__dict__['sumsq_timestamp'])\n","print (cluster_reps[3].reprs['cluster'])\n","\n","arr = cluster_reps[3].reprs['body']\n","\n","print (arr[utils.np.nonzero(arr)])\n","print (cluster_reps[3].__dict__['num_docs'])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"57M1HCJ_fdkB"},"outputs":[],"source":["#cluster_reps= pd.read_pickle(\"POC/models/dev_cluster_pool_2KJul30.pkl\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aAOjXtrAfdkB"},"outputs":[],"source":["#training_reps= pd.read_pickle(\"POC/models/dev_training_reps_2KJul30.pkl\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AOtz7hvufdkB"},"outputs":[],"source":["print (len(cluster_reps))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lhajfDRPfdkB"},"outputs":[],"source":["print (len(training_reps))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6wLOcazqfdkC"},"outputs":[],"source":["training_reps[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9TPj49_ufdkC"},"outputs":[],"source":["cluster_reps[0].clustid"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MY2EW4WIfdkC"},"outputs":[],"source":["clusters = {rep.clustid for rep in cluster_reps}\n","print (len(clusters))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qi7WG9rMfdkC"},"outputs":[],"source":["print (clusters)"]},{"cell_type":"markdown","metadata":{"id":"mbRvhg23fdkC"},"source":["Get the triplet loss value by the similarity differences between the anchor and positive cluster and anchor and negative cluster"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zk8PSB_yfdkD"},"outputs":[],"source":["import numpy as np\n","import random\n","from tqdm import tqdm_notebook as tqdm\n","tqdm().pandas()\n","import random\n","\n","instances = []\n","with tqdm(total=len(clusters)) as pbar:\n","\n","  for c in clusters:\n","    doclist = list(filter(lambda clst: clst.reprs['cluster'] == c, training_reps))\n","    target_cluster = list(filter(lambda clst: clst.reprs['cluster'] == c, cluster_reps))\n","    #ensure the cluster representation is unique\n","    assert (len(target_cluster) == 1)\n","    for anchor in doclist:\n","        print ('\\nPostive sample with cluster ',target_cluster[0].reprs['cluster'],' and anchor doc cluster ', anchor.reprs['cluster'])\n","        sims = utils.sim_reps_dc(anchor, target_cluster[0] )\n","        \n","        sim_vec = utils.create_vec(sims)\n","        print (sim_vec)\n","        neg_cluster = random.choice(cluster_reps)\n","        \n","        while (neg_cluster.reprs['cluster'] == c):\n","            neg_cluster = random.choice(cluster_reps)\n","            \n","        print ('Negative sample with cluster ',neg_cluster.reprs['cluster'],' and anchor doc cluster ', anchor.reprs['cluster'])\n","        difs = utils.sim_reps_dc(anchor, neg_cluster)\n","        dif_vec = utils.create_vec(difs)\n","        print (dif_vec)\n","\n","        loss = np.subtract(sim_vec, dif_vec)\n","        print ('Their difference: ', loss)\n","        instances.append([loss, 1])\n","    pbar.update(1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uX1eTtjPfdkD"},"outputs":[],"source":["print (instances[2])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KUdfb5HffdkD"},"outputs":[],"source":["print (len(instances))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2GlLfWiufdkD"},"outputs":[],"source":["train_df = pd.DataFrame.from_records(instances, columns=['features', 'labels'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n_X-p3wbfdkD"},"outputs":[],"source":["train_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HcTi8xh9fdkE"},"outputs":[],"source":["train_len = (len(train_df))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T-N0pJ8IfdkE"},"outputs":[],"source":["print (train_len)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z5parz0-fdkE"},"outputs":[],"source":["half = train_len*0.5"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3XFCrrBjfdkE"},"outputs":[],"source":["train_df.loc[half:train_len-1,'labels'] =0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cZui6qv6fdkE"},"outputs":[],"source":["train_df.groupby('labels').count()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xqG155_XfdkE"},"outputs":[],"source":["train_df.tail()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"32RYHrJVfdkE"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(train_df.features, train_df.labels, test_size=0.2, random_state=109) "]},{"cell_type":"markdown","metadata":{"id":"QGeuoUQUfdkF"},"source":["Train linear classifier using the loss instances on an artificially balanced dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LnZABrMffdkF"},"outputs":[],"source":["from sklearn import svm\n","clf = svm.SVC(kernel='linear')\n","clf.fit(list(X_train), list(y_train))\n","y_pred = clf.predict(list(X_test))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g9t2RQa6fdkF"},"outputs":[],"source":["from sklearn import metrics\n","\n","# Model Accuracy: how often is the classifier correct?\n","print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7DdAS0lifdkF"},"outputs":[],"source":["# Model Precision: what percentage of positive tuples are labeled as such?\n","print(\"Precision:\",metrics.precision_score(y_test, y_pred))\n","\n","# Model Recall: what percentage of positive tuples are labelled as such?\n","print(\"Recall:\",metrics.recall_score(y_test, y_pred))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"70u1zI6MfdkF"},"outputs":[],"source":["filename = 'POC/models/SVM_similaritymodel_Aug4.svm'\n","utils.pickle.dump(clf, open(filename, 'wb'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ajQSs5FffdkF"},"outputs":[],"source":["print (model.model_reps + model.time_reps)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kfENh6IdfdkF"},"outputs":[],"source":["print (dict(zip (model.model_reps + model.time_reps, clf.coef_[0])))"]},{"cell_type":"markdown","metadata":{"id":"OFGlgja2fdkG"},"source":["Train separate Tf-Idf models for variations of the text corpus. These variations are tokens, lemmas, and entities occurring in the entire corpus. \n","These are used to encode into representations vectors for title, body, title body - and their lemma and entity variations"]}],"metadata":{"kernelspec":{"display_name":"eventclustering","language":"python","name":"eventclustering"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"colab":{"name":"train_tripletloss.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}