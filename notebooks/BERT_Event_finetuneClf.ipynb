{"cells":[{"cell_type":"markdown","metadata":{"id":"c4rTDE1-Cd0K"},"source":["<b>CBERT TRAINING</b>  \n","Mulitlabel (~200 labels) classifier with distiliBert classify event membership of news items in BERT finetuning task\n","\n","Trained on News Clustering corpus"]},{"cell_type":"code","source":["!pip install simpletransformers\n","!pip install sentence_transformers\n","!pip install bert-tensorflow"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UVdLv4ZZKKcq","executionInfo":{"status":"ok","timestamp":1659756244469,"user_tz":240,"elapsed":14491,"user":{"displayName":"Nadia Conroy","userId":"09687839967270010584"}},"outputId":"4cb0b22d-68f8-471c-8086-6082e09fa486"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: simpletransformers in /usr/local/lib/python3.7/dist-packages (0.63.7)\n","Requirement already satisfied: tokenizers in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (0.12.1)\n","Requirement already satisfied: streamlit in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (1.11.1)\n","Requirement already satisfied: wandb>=0.10.32 in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (0.13.1)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (1.3.5)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (0.1.96)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (2.4.0)\n","Requirement already satisfied: tqdm>=4.47.0 in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (4.64.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (2.23.0)\n","Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (2022.6.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (1.21.6)\n","Requirement already satisfied: seqeval in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (1.2.2)\n","Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (2.8.0)\n","Requirement already satisfied: transformers>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (4.21.1)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (1.0.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (1.7.3)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.6.0->simpletransformers) (0.8.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.6.0->simpletransformers) (6.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers>=4.6.0->simpletransformers) (4.12.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>=4.6.0->simpletransformers) (3.7.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.6.0->simpletransformers) (21.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers>=4.6.0->simpletransformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers>=4.6.0->simpletransformers) (3.0.9)\n","Requirement already satisfied: protobuf<4.0dev,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb>=0.10.32->simpletransformers) (3.17.3)\n","Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb>=0.10.32->simpletransformers) (2.3)\n","Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb>=0.10.32->simpletransformers) (7.1.2)\n","Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb>=0.10.32->simpletransformers) (1.15.0)\n","Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb>=0.10.32->simpletransformers) (0.1.2)\n","Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb>=0.10.32->simpletransformers) (0.4.0)\n","Requirement already satisfied: setproctitle in /usr/local/lib/python3.7/dist-packages (from wandb>=0.10.32->simpletransformers) (1.3.1)\n","Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb>=0.10.32->simpletransformers) (3.1.27)\n","Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb>=0.10.32->simpletransformers) (1.0.9)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb>=0.10.32->simpletransformers) (57.4.0)\n","Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb>=0.10.32->simpletransformers) (5.4.8)\n","Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb>=0.10.32->simpletransformers) (1.9.0)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb>=0.10.32->simpletransformers) (4.0.9)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb>=0.10.32->simpletransformers) (5.0.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->simpletransformers) (1.25.11)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->simpletransformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->simpletransformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->simpletransformers) (2022.6.15)\n","Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets->simpletransformers) (6.0.1)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets->simpletransformers) (0.70.13)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets->simpletransformers) (3.0.0)\n","Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.7/dist-packages (from datasets->simpletransformers) (0.18.0)\n","Requirement already satisfied: dill<0.3.6 in /usr/local/lib/python3.7/dist-packages (from datasets->simpletransformers) (0.3.5.1)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets->simpletransformers) (3.8.1)\n","Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.7/dist-packages (from datasets->simpletransformers) (2022.7.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->simpletransformers) (22.1.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->simpletransformers) (6.0.2)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->simpletransformers) (1.8.1)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->simpletransformers) (2.1.0)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->simpletransformers) (4.0.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->simpletransformers) (1.3.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->simpletransformers) (1.2.0)\n","Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->simpletransformers) (0.13.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers>=4.6.0->simpletransformers) (3.8.1)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->simpletransformers) (2022.1)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->simpletransformers) (2.8.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->simpletransformers) (3.1.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->simpletransformers) (1.1.0)\n","Requirement already satisfied: altair>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers) (4.2.0)\n","Requirement already satisfied: semver in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers) (2.13.0)\n","Requirement already satisfied: tzlocal>=1.1 in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers) (1.5.1)\n","Requirement already satisfied: pydeck>=0.1.dev5 in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers) (0.7.1)\n","Requirement already satisfied: blinker>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers) (1.5)\n","Requirement already satisfied: cachetools>=4.0 in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers) (4.2.4)\n","Requirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers) (0.10.2)\n","Requirement already satisfied: watchdog in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers) (2.1.9)\n","Requirement already satisfied: pympler>=0.9 in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers) (1.0.1)\n","Requirement already satisfied: tornado>=5.0 in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers) (6.2)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers) (7.1.2)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers) (12.5.1)\n","Requirement already satisfied: validators>=0.2 in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers) (0.20.0)\n","Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit->simpletransformers) (4.3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit->simpletransformers) (2.11.3)\n","Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit->simpletransformers) (0.4)\n","Requirement already satisfied: toolz in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit->simpletransformers) (0.12.0)\n","Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit->simpletransformers) (5.9.0)\n","Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit->simpletransformers) (0.18.1)\n","Requirement already satisfied: traitlets>=4.3.2 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit->simpletransformers) (5.1.1)\n","Requirement already satisfied: ipykernel>=5.1.2 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit->simpletransformers) (6.15.1)\n","Requirement already satisfied: ipywidgets>=7.0.0 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit->simpletransformers) (7.7.1)\n","Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (1.0.0)\n","Requirement already satisfied: ipython>=7.23.1 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (7.34.0)\n","Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (23.2.0)\n","Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (1.5.5)\n","Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (7.3.4)\n","Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.1.3)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (2.6.1)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (4.4.2)\n","Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (4.8.0)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.7.5)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.2.0)\n","Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.18.1)\n","Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (3.0.30)\n","Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (3.6.1)\n","Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (1.1.1)\n","Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.2.0)\n","Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.8.3)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->altair>=3.2.0->streamlit->simpletransformers) (2.0.1)\n","Requirement already satisfied: jupyter-core>=4.9.2 in /usr/local/lib/python3.7/dist-packages (from jupyter-client>=6.1.12->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (4.11.1)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.7.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.2.5)\n","Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from rich>=10.11.0->streamlit->simpletransformers) (0.9.1)\n","Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (5.3.1)\n","Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.13.3)\n","Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (1.8.0)\n","Requirement already satisfied: nbformat in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (5.4.0)\n","Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (5.6.1)\n","Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.8.4)\n","Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (5.0.1)\n","Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (1.5.0)\n","Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.6.0)\n","Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.7.1)\n","Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (2.16.1)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.5.1)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->simpletransformers) (1.35.0)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard->simpletransformers) (0.37.1)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->simpletransformers) (0.4.6)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->simpletransformers) (0.6.1)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->simpletransformers) (1.47.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard->simpletransformers) (3.4.1)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard->simpletransformers) (1.2.0)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->simpletransformers) (1.8.1)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard->simpletransformers) (1.0.1)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->simpletransformers) (0.2.8)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->simpletransformers) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->simpletransformers) (1.3.1)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->simpletransformers) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->simpletransformers) (3.2.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.7/dist-packages (2.2.2)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.13.0+cu113)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.7.3)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (3.7)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.21.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (4.64.0)\n","Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.8.1)\n","Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (4.21.1)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.0.2)\n","Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.12.0+cu113)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.1.96)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.12.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.1.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (21.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.7.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence_transformers) (3.0.9)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2022.6.2)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.12.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->huggingface-hub>=0.4.0->sentence_transformers) (3.8.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk->sentence_transformers) (1.1.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk->sentence_transformers) (7.1.2)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2022.6.15)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (1.25.11)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence_transformers) (3.1.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence_transformers) (7.1.2)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting bert-tensorflow\n","  Downloading bert_tensorflow-1.0.4-py2.py3-none-any.whl (64 kB)\n","\u001b[K     |████████████████████████████████| 64 kB 2.3 MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from bert-tensorflow) (1.15.0)\n","Installing collected packages: bert-tensorflow\n","Successfully installed bert-tensorflow-1.0.4\n"]}]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JjOjD0PKCd0M","executionInfo":{"status":"ok","timestamp":1659756244470,"user_tz":240,"elapsed":19,"user":{"displayName":"Nadia Conroy","userId":"09687839967270010584"}},"outputId":"377cf110-6969-4ec6-a630-bb6ac66977d6"},"outputs":[{"output_type":"stream","name":"stdout","text":["True\n","1.12.0+cu113\n"]}],"source":["from simpletransformers.classification import MultiLabelClassificationModel\n","\n","import pandas as pd\n","from sentence_transformers import SentenceTransformer, InputExample, losses\n","from torch.utils.data import DataLoader\n","#import utils\n","import torch\n","print (torch.cuda.is_available())\n","print (torch.__version__)"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AfJ35OOCI_LA","executionInfo":{"status":"ok","timestamp":1659756247741,"user_tz":240,"elapsed":984,"user":{"displayName":"Nadia Conroy","userId":"09687839967270010584"}},"outputId":"750772e2-4676-4492-d648-c3e46a106ccb"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","execution_count":10,"metadata":{"id":"b3ee24bf","executionInfo":{"status":"ok","timestamp":1659756249701,"user_tz":240,"elapsed":133,"user":{"displayName":"Nadia Conroy","userId":"09687839967270010584"}}},"outputs":[],"source":["import sys\n","sys.path.insert(1, './drive/MyDrive/EventClustering/src')"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"Tz-rxMu-Cd0M","executionInfo":{"status":"ok","timestamp":1659756253471,"user_tz":240,"elapsed":1077,"user":{"displayName":"Nadia Conroy","userId":"09687839967270010584"}}},"outputs":[],"source":["#Define the model. Either from scratch of by loading a pre-trained model\n","model = SentenceTransformer('distilbert-base-nli-mean-tokens')"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":433},"id":"KNquBt3wCd0N","executionInfo":{"status":"error","timestamp":1659756256322,"user_tz":240,"elapsed":522,"user":{"displayName":"Nadia Conroy","userId":"09687839967270010584"}},"outputId":"f5de5332-8569-40d1-d910-de77fd726883"},"outputs":[{"output_type":"stream","name":"stdout","text":["Pytorch Version: 1.12.0+cu113\n"]},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-f1147a9d765e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'eng'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest_corpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCorpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m test_corpus.load_corpora(r\"/drive/MyDrive/dataset/EventClustering/dataset.dev.json\",\n\u001b[1;32m      5\u001b[0m                         r\"./drive/MyDrive/dataset/EventClustering/clustering.dev.json\", set([lang]))\n","\u001b[0;32m/content/drive/MyDrive/EventClustering/src/corpus.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mCorpus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/EventClustering/src/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0mpretrained_bertpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./models/BertFinetune/clustFT-bert-base-uncased\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_bertpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_LEN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0mbert_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertForSequenceClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_bertpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0;31m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 534\u001b[0;31m         \u001b[0mtokenizer_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_tokenizer_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    535\u001b[0m         \u001b[0mconfig_tokenizer_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tokenizer_class\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m         \u001b[0mtokenizer_auto_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mget_tokenizer_config\u001b[0;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, **kwargs)\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m         \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m         \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m     )\n\u001b[1;32m    403\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresolved_config_file\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mget_file_from_repo\u001b[0;34m(path_or_repo, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only)\u001b[0m\n\u001b[1;32m    709\u001b[0m             \u001b[0mresume_download\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_download\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 711\u001b[0;31m             \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    712\u001b[0m         )\n\u001b[1;32m    713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m    290\u001b[0m             \u001b[0muser_agent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muser_agent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m             \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m         )\n\u001b[1;32m    294\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_or_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m    561\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m                     raise ValueError(\n\u001b[0;32m--> 563\u001b[0;31m                         \u001b[0;34m\"Connection error, and we cannot find the requested files in the cached path.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m                         \u001b[0;34m\" Please try again or make sure your Internet connection is on.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m                     )\n","\u001b[0;31mValueError\u001b[0m: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on."]}],"source":["import corpus\n","lang = 'eng'\n","test_corpus = corpus.Corpus()\n","test_corpus.load_corpora(r\"/drive/MyDrive/dataset/EventClustering/dataset.dev.json\",\n","                        r\"./drive/MyDrive/dataset/EventClustering/clustering.dev.json\", set([lang]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JWTUGVSHCd0N"},"outputs":[],"source":["input_df=pd.DataFrame(test_corpus.documents)\n","input_df=input_df[['text','event_id']]\n","print (input_df.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KoiS4jqrCd0N"},"outputs":[],"source":["input_df['text'] = input_df['text'].apply(lambda x: str(x).replace('\\n', ' '))\n","\n","input_df['text'] = input_df['text'].apply(lambda x: utils.preprocess_text(x))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JjsZVodFCd0N"},"outputs":[],"source":["input_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pan9hFRnCd0O"},"outputs":[],"source":["from sklearn.preprocessing import OneHotEncoder\n","# creating instance of one-hot-encoder\n","enc = OneHotEncoder(handle_unknown='ignore')\n","\n","input_df['l_tmp'] =input_df['event_id'].astype('category')\n","input_df['labels_cat'] = input_df['l_tmp'].cat.codes\n","enc_df = pd.DataFrame(list(zip(enc.fit_transform(input_df[['labels_cat']]).toarray())), columns = ['labels'])\n","enc_df['text'] = input_df['text']\n","enc_df.head()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SnE13C1zCd0O"},"outputs":[],"source":["sentences = (enc_df.text.values)\n","labels = (enc_df.labels.values)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fnCc53hNCd0O"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","train_df, eval_df = train_test_split(enc_df, random_state=2018, test_size=0.2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LTDVweU4Cd0O"},"outputs":[],"source":["train_df = pd.DataFrame(train_df)\n","eval_df = pd.DataFrame(eval_df)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-z6-CiyRCd0O"},"outputs":[],"source":["train_df.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K9TvZ1b5Cd0P"},"outputs":[],"source":["from sentence_transformers import evaluation\n","\n","def similarity_losstrain():\n","    #Define your train examples. You need more than just two examples...\n","    train_examples = [InputExample(texts=[raw_data['text_a'], raw_data['text_b']], label=raw_data['similarity'])]\n","    evaluator = evaluation.EmbeddingSimilarityEvaluator(sentences1=eval_df['text_a'], sentences2=eval_df['text_b'], scores=eval_df['similarity'], batch_size=32)\n","    #Define your train dataset, the dataloader and the train loss\n","    train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=32)\n","    train_loss = losses.BatchHardTripletLoss(model=model)\n","\n","    #Tune the model\n","    model.fit(train_objectives=[(train_dataloader, train_loss)],  epochs=4, warmup_steps=100)"]},{"cell_type":"markdown","metadata":{"id":"9vNu_krICd0P"},"source":["Create One-Hot vectors for the categorical labels to prepare for multilabel cluster fine tuning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8CujGVt0Cd0P"},"outputs":[],"source":["eval_df.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e2rZOyFtCd0P"},"outputs":[],"source":["output_dir ='models/BERT_ClusterMember_finetune'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LlFbXj7JCd0P"},"outputs":[],"source":["model_args = {\n","    \"num_train_epochs\": 2,\n","    \"learning_rate\": 2e-5,\n","    'reprocess_input_data': True,\n","    'train_batch_size':4, \n","    'gradient_accumulation_steps':8, \n","    'learning_rate': 2e-5, \n","    'scheduler': \"linear_schedule_with_warmup\",\n","    'optimizer': 'AdamW',\n","    #'metric': metric,\n","    'use_multiprocessing': True,\n","    'tensorboard_dir': \"tensorboard\",\n","    'weight_decay':  0.02,\n","    'adam_epsilon':  2e-8,                                           \n","    'overwrite_output_dir': True, \n","    'fp16': False,\n","    'max_seq_length': 128,\n","    'save_model_every_epoch': False,\n","    'output_dir': output_dir\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gdn1A4oBCd0Q"},"outputs":[],"source":["num_labels=input_df['event_id'].nunique()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Unb4FBQ_Cd0Q"},"outputs":[],"source":["model = MultiLabelClassificationModel('roberta', \n","                                      'roberta-base',  \n","                                      use_cuda=True, \n","                                      num_labels=num_labels, \n","                                      args=model_args\n","                                     )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g41VeHCBCd0Q"},"outputs":[],"source":["print (num_labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"dgPTyzp6Cd0Q"},"outputs":[],"source":["model.train_model(train_df)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gOgbOV5eCd0Q"},"outputs":[],"source":["enc_df.head()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k2295QTcCd0Q"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gG9BdBC1Cd0R"},"outputs":[],"source":["from transformers import BertTokenizer\n","\n","# Load BERT tokenizer.\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uqIxxwM4Cd0R"},"outputs":[],"source":["print(' Original: ', sentences[0])\n","print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n","print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PbB87dnFCd0R"},"outputs":[],"source":["# Tokenize all of the sentences and map the tokens to thier word IDs.\n","input_ids = []\n","\n","# For every sentence...\n","for sent in sentences:\n","    #   (1) Tokenize the sentence.\n","    #   (2) Prepend the `[CLS]` token to the start.\n","    #   (3) Append the `[SEP]` token to the end.\n","    #   (4) Map tokens to their IDs.\n","    encoded_sent = tokenizer.encode(\n","                        sent,                      \n","                        add_special_tokens = True, \n","                      )\n","    # Add the encoded sentence to the list.\n","    input_ids.append(encoded_sent)\n","\n","# Print sentence 0, now as a list of IDs.\n","print('Original: ', sentences[0])\n","print('Token IDs:', input_ids[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7ovb5O6MCd0R"},"outputs":[],"source":["print('Max sentence length: ', max([len(sen) for sen in input_ids]))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p6TwXU8KCd0R"},"outputs":[],"source":["from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n","MAX_LEN = 80\n","print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n","\n","print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n","\n","# Pad our input tokens with value 0.\n","input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n","                          value=0, truncating=\"post\", padding=\"post\")\n","print('\\nDone.')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M7_A1pudCd0R"},"outputs":[],"source":["# Create attention masks\n","attention_masks = []\n","for sent in input_ids:\n","    \n","    # Create the attention mask.\n","    #   - If a token ID is 0, then it's padding, set the mask to 0.\n","    #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n","    att_mask = [int(token_id > 0) for token_id in sent]\n","    \n","    # Store the attention mask for this sentence.\n","    attention_masks.append(att_mask)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OTzSjAXpCd0S"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n","                                                            random_state=2018, test_size=0.2)\n","\n","train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels,\n","                                             random_state=2018, test_size=0.2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iRYopHcLCd0S"},"outputs":[],"source":["print (len(train_labels))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pQJp0kJ8Cd0S"},"outputs":[],"source":["train_labels = torch.from_numpy(np.ndarray(train_labels))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uOtxd3nvCd0S"},"outputs":[],"source":["# Convert all inputs and labels into torch tensors\n","import numpy as np\n","train_inputs = torch.tensor(train_inputs)\n","validation_inputs = torch.tensor(validation_inputs)\n","\n","train_labels= torch.tensor(train_labels)\n","validation_labels = torch.tensor(validation_labels)\n","\n","train_masks = torch.tensor(train_masks)\n","validation_masks = torch.tensor(validation_masks)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZJFJwExxCd0S"},"outputs":[],"source":["from transformers import BertForSequenceClassification, AdamW, BertConfig\n","\n","# Load BertForSequenceClassification, the pretrained BERT model with a single \n","# linear classification layer on top. \n","model = BertForSequenceClassification.from_pretrained(\n","    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n","    num_labels = 2, \n","    output_attentions = False, # Whether the model returns attentions weights.\n","    output_hidden_states = False, # Whether the model returns all hidden-states.\n",")\n","\n","model.cuda()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uSzlj5kxCd0S"},"outputs":[],"source":["params = list(model.named_parameters())\n","print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n","\n","print('==== Embedding Layer ====\\n')\n","\n","for p in params[0:5]:\n","    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n","\n","print('\\n==== First Transformer ====\\n')\n","\n","for p in params[5:21]:\n","    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n","\n","print('\\n==== Output Layer ====\\n')\n","\n","for p in params[-4:]:\n","    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3y814UaACd0T"},"outputs":[],"source":["no_decay = ['bias', 'LayerNorm.weight']\n","\n","# Separate the `weight` parameters from the `bias` parameters. \n","# - For the `weight` parameters, this specifies a 'weight_decay_rate' of 0.01. \n","# - For the `bias` parameters, the 'weight_decay_rate' is 0.0. \n","optimizer_grouped_parameters = [\n","    # Filter for all parameters which *don't* include 'bias', 'gamma', 'beta'.\n","    {'params': [p for n, p in model.named_parameters()  if not any(nd in n for nd in no_decay)],\n","     'weight_decay_rate': 0.1},\n","    \n","    # Filter for parameters which *do* include those.\n","    {'params': [p for n, p in model.named_parameters()  if any(nd in n for nd in no_decay)],\n","     'weight_decay_rate': 0.0}\n","]\n","\n","# Adam optimizer\n","optimizer = AdamW(optimizer_grouped_parameters,\n","                  lr = 2e-5, # args.learning_rate \n","                  eps = 1e-8 # args.adam_epsilon\n","                )\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"85ajVN1sCd0T"},"outputs":[],"source":["from transformers import get_linear_schedule_with_warmup\n","\n","epochs = 2\n","\n","# Training steps = batches * number of epochs.\n","total_steps = len(train_dataloader) * epochs\n","scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                            num_warmup_steps = 0, # Default value in run_glue.py\n","                                            num_training_steps = total_steps)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s_TVEfByCd0T"},"outputs":[],"source":["import numpy as np\n","\n","def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rdfnnvGXCd0T"},"outputs":[],"source":["import time\n","import datetime\n","\n","def format_time(elapsed):\n","    elapsed_rounded = int(round((elapsed)))\n","    return str(datetime.timedelta(seconds=elapsed_rounded))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QrHUPJApCd0T"},"outputs":[],"source":["import random\n","\n","# This training code is based on the `run_glue.py` \n","# set the seed value everywhere\n","seed_val = 42\n","device='cuda'\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","# Store the average loss after each epoch so we can plot them.\n","loss_values = []\n","\n","for epoch_i in range(0, epochs):\n","\n","    print(\"\")\n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","    print('Training...')\n","\n","    # Measure how long the training epoch takes.\n","    t0 = time.time()\n","\n","    # Reset the total loss for this epoch.\n","    total_loss = 0\n","\n","    model.train()\n","\n","    # For each batch of training data...\n","    for step, batch in enumerate(train_dataloader):\n","\n","        # Progress update every 40 batches.\n","        if step % 40 == 0 and not step == 0:\n","            # Calculate elapsed time in minutes.\n","            elapsed = format_time(time.time() - t0)\n","            \n","            # Report progress.\n","            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n","            \n","        # `batch` contains three pytorch tensors:\n","        #   [0]: input ids \n","        #   [1]: attention masks\n","        #   [2]: labels \n","        b_input_ids = batch[0].to(device)\n","\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","        \n","        # Always clear any previously calculated gradients before performing a\n","        # backward pass. \n","        model.zero_grad()        \n","        print (\"THE INPUT IDS: \",b_input_ids.long())\n","        print (\"ATTENTION MASK: \",b_input_mask.long())\n","        print (\"LABELS: \", b_labels.long())\n","        # Perform a forward pass (evaluate the model on this training batch).\n","        # This will return the loss \n","        outputs = model(b_input_ids.long(), \n","                    token_type_ids=None, \n","                    attention_mask=b_input_mask, \n","                    labels=b_labels.long())\n","        \n","        # pull the loss value out of the tuple.\n","        loss = outputs[0]\n","\n","        # Accumulate the training loss over all of the batches .\n","        total_loss += loss.item()\n","\n","        # Perform a backward pass to calculate the gradients.\n","        loss.backward()\n","\n","        # Stop exploding gradients, Clip the norm of the gradients to 1.0.\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","        # Update parameters and take a step using the computed gradient.\n","        optimizer.step()\n","\n","        # Update the learning rate.\n","        scheduler.step()\n","\n","    # Calculate the average loss over the training data.\n","    avg_train_loss = total_loss / len(train_dataloader)            \n","    \n","    # Store the loss value for plotting the learning curve.\n","    loss_values.append(avg_train_loss)\n","\n","    print(\"\")\n","    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n","        \n","    #  Validation Step\n","\n","    print(\"\")\n","    print(\"Running Validation...\")\n","\n","    t0 = time.time()\n","\n","    # Put the model in evaluation mode--the dropout layers behave differently\n","    # during evaluation.\n","    model.eval()\n","\n","    # Tracking variables \n","    eval_loss, eval_accuracy = 0, 0\n","    nb_eval_steps, nb_eval_examples = 0, 0\n","\n","    # Evaluate data for one epoch\n","    for batch in validation_dataloader:\n","        \n","        # Add batch to GPU\n","        batch = tuple(t.to(device) for t in batch)\n","        \n","        # Unpack the inputs from our dataloader\n","        b_input_ids, b_input_mask, b_labels = batch\n","\n","        # Do not compute or store gradients to save space\n","        with torch.no_grad():        \n","  \n","            # Forward pass, calculate logit predictions.\n","            outputs = model(b_input_ids.long(), \n","                            token_type_ids=None, \n","                            attention_mask=b_input_mask.long())\n","        \n","        # Get the \"logits\" output by the model.\n","        logits = outputs[0]\n","        print (\"LOGITS ARE: \", logits)\n","        # Move logits and labels to CPU\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","        \n","        # Calculate the accuracy for this batch of test sentences.\n","        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n","        \n","        # Accumulate the total accuracy.\n","        eval_accuracy += tmp_eval_accuracy\n","\n","        # Track the number of batches\n","        nb_eval_steps += 1\n","\n","    # Report the final accuracy for this validation run.\n","    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n","    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n","\n","print(\"\")\n","print(\"Training complete!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qr76d6lYCd0U"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","import seaborn as sns\n","\n","# Use plot styling from seaborn.\n","sns.set(style='darkgrid')\n","\n","# Increase the plot size and font size.\n","sns.set(font_scale=1.5)\n","plt.rcParams[\"figure.figsize\"] = (12,6)\n","\n","# Plot the learning curve.\n","plt.plot(loss_values, 'b-o')\n","\n","# Label the plot.\n","plt.title(\"Training loss\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Slu6LNYpCd0U"},"outputs":[],"source":["import os\n","\n","# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n","\n","output_dir = './outputs_man/'\n","\n","# Create output directory if needed\n","if not os.path.exists(output_dir):\n","    os.makedirs(output_dir)\n","\n","print(\"Saving model to %s\" % output_dir)\n","\n","# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n","# They can then be reloaded using `from_pretrained()`\n","model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n","model_to_save.save_pretrained(output_dir)\n","tokenizer.save_pretrained(output_dir)"]}],"metadata":{"kernelspec":{"display_name":"eventclustering","language":"python","name":"eventclustering"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"colab":{"name":"BERT_Event_finetuneClf.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU","gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}