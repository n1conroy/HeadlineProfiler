{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173aca3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import corpus\n",
    "import random\n",
    "import spacy\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "max_seq_length=1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6095887d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm') \n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', max_length=512)\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased',\n",
    "                                  output_hidden_states = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d7745e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = 'eng'\n",
    "dataset = corpus.Corpus()\n",
    "dataset.load_corpora(r\"../news-clustering/dataset/dataset.test.json\",\n",
    "                     r\"../news-clustering/dataset/clustering.test.json\", set([lang]))\n",
    "input_data = dataset.documents\n",
    "random.shuffle(input_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b890796b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entity_spacy(sentence):\n",
    "    doc = nlp(''.join(str(sentence)))\n",
    "    ents = [ent.text for ent in doc.ents]  \n",
    "    #return \" \".join(ents)\n",
    "    return list(set(ents))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bbed36",
   "metadata": {},
   "source": [
    "<b>after identifying the entities in the text, locate the index positions of the tokens and replace entity terms with the [MASK] token. Encode the rest of the input with the [SEP] [CLS] tokens. Return the list of changed indices</b> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e678e72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_indeces(tokenizer, text, word):\n",
    "\n",
    "    word_tokens = tokenizer.tokenize(word)\n",
    "    masks_str = ' '.join(['[MASK]']*len(word_tokens))\n",
    "    text_masked = text.replace(word, masks_str)\n",
    "\n",
    "    input_ids = tokenizer.encode(text_masked)\n",
    "    mask_token_indeces = np.where(np.array(input_ids) == tokenizer.mask_token_id)[0]\n",
    "\n",
    "    return mask_token_indeces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0121e567",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_maskembedding(b_model, b_tokenizer, text, ents=''):\n",
    "    '''\n",
    "    Uses the provided model and tokenizer to produce an embedding for the\n",
    "    provided `text`, and a \"contextualized\" embedding for `word`, if provided.\n",
    "    '''\n",
    "\n",
    "    # If entities are provided, figure out which tokens correspond to it.\n",
    "    print ('\\n text is ', text)\n",
    "    if not ents == '':\n",
    "        word_indeces = []\n",
    "        for e in ents:\n",
    "            i = get_word_indeces(b_tokenizer, text, e)\n",
    "            word_indeces.extend(i)\n",
    "        word_indeces.sort()\n",
    "    print ('\\n ents are ', ents)\n",
    "    print ('\\n indices are ', word_indeces)\n",
    "    # Encode the text, adding the (required!) special tokens, and converting to\n",
    "    # PyTorch tensors.\n",
    "    encoded_dict = b_tokenizer.encode_plus(\n",
    "                        text,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # zxAdd '[CLS]' and '[SEP]'\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                        max_length=512)\n",
    "\n",
    "    print ('\\n encoded is', encoded_dict)\n",
    "    input_ids = encoded_dict['input_ids']\n",
    "    \n",
    "    b_model.eval()\n",
    "\n",
    "    # Run the text through the model and get the hidden states.\n",
    "    bert_outputs = b_model(input_ids)\n",
    "    \n",
    "    # Run the text through BERT, and collect all of the hidden states produced\n",
    "    # from all 12 layers. \n",
    "    with torch.no_grad():\n",
    "\n",
    "        outputs = b_model(input_ids)\n",
    "\n",
    "        # Evaluating the model will return a different number of objects based on \n",
    "        # how it's  configured in the `from_pretrained` call earlier. In this case, \n",
    "        # becase we set `output_hidden_states = True`, the third item will be the \n",
    "        # hidden states from all layers. See the documentation for more details:\n",
    "        # https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n",
    "        hidden_states = outputs[2]\n",
    "\n",
    "    # `hidden_states` has shape [13 x 1 x <sentence length> x 768]\n",
    "\n",
    "    # Select the embeddings from the second to last layer.\n",
    "    # `token_vecs` is a tensor with shape [<sent length> x 768]\n",
    "    token_vecs = hidden_states[-2][0]\n",
    "\n",
    "    # Calculate the average of all token vectors.\n",
    "    sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "\n",
    "    # Convert to numpy array.\n",
    "    sentence_embedding = sentence_embedding.detach().numpy()\n",
    "\n",
    "    # If `word` was provided, compute an embedding for those tokens.\n",
    "    '''\n",
    "    if not ents == '':\n",
    "        word_embedding = torch.mean(token_vecs[word_indeces], dim=0)\n",
    "        word_embedding = word_embedding.detach().numpy()\n",
    "    \n",
    "        return (sentence_embedding, word_embedding)\n",
    "    else:\n",
    "        return sentence_embedding\n",
    "    '''\n",
    "    print ('sentence_embedding', sentence_embedding)\n",
    "    return sentence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d720bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = input_data[:10]\n",
    "for i, d in enumerate(input_data):\n",
    "    x = corpus.Document(d)\n",
    "    print ('\\nINSIDE DOCUMENT ', x.id)\n",
    "    ents  = get_entity_spacy(x.body[:max_seq_length])\n",
    "    get_maskembedding(bert_model, bert_tokenizer, x.body[:max_seq_length], ents=ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3705d8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eventclustering",
   "language": "python",
   "name": "eventclustering"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
